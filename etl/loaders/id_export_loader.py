import requests
import gzip
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from base_loader import BaseLoader
from tmdb_client import TMDBClient

class IDExportLoader:
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ ID –∏–∑ TMDB.
    
    –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–æ–≤:
    - NDJSON (Newline Delimited JSON) - –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —ç—Ç–æ JSON –æ–±—ä–µ–∫—Ç
    - Gzipped
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {"adult":false,"id":123,"original_title":"..","popularity":45.2,"video":false}
    """
    
    BASE_URL = "https://files.tmdb.org/p/exports"
    
    def __init__(self, media_type: str = "movie"):
        """
        Args:
            media_type: "movie", "tv_series", "person", "collection" –∏ —Ç.–¥.
        """
        self.media_type = media_type
        self.data_dir = os.getenv("DATA_DIR", "./data")
        os.makedirs(self.data_dir, exist_ok=True)
    
    def _get_latest_filename(self, days_back: int = 0) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –¥–∞—Ç—ã.
        TMDB –ø—É–±–ª–∏–∫—É–µ—Ç —Ñ–∞–π–ª—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ: movie_ids_MM_DD_YYYY.json.gz
        """
        date = datetime.now() - timedelta(days=days_back)
        date_str = date.strftime("%m_%d_%Y")
        return f"{self.media_type}_ids_{date_str}.json.gz"
    
    def download_export(self, max_retries: int = 7) -> Optional[str]:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π export —Ñ–∞–π–ª.
        TMDB —Ö—Ä–∞–Ω–∏—Ç —Ñ–∞–π–ª—ã —Ç–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –º–µ—Å—è—Ü–∞.
        
        Returns:
            –ü—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None
        """
        # –ü—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π (—Ñ–∞–π–ª—ã –ø—É–±–ª–∏–∫—É—é—Ç—Å—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ ~8 AM UTC)
        for days_back in range(max_retries):
            filename = self._get_latest_filename(days_back)
            url = f"{self.BASE_URL}/{filename}"
            local_path = os.path.join(self.data_dir, filename)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if os.path.exists(local_path):
                print(f"‚úì Using cached file: {filename}")
                return local_path
            
            print(f"Trying to download: {url}")
            
            try:
                response = requests.get(url, stream=True, timeout=30)
                
                if response.status_code == 200:
                    # –°–∫–∞—á–∏–≤–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
                    file_size = int(response.headers.get('content-length', 0))
                    
                    print(f"Downloading {filename} ({file_size / 1024 / 1024:.2f} MB)...")
                    
                    with open(local_path, 'wb') as f:
                        downloaded = 0
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å
                            if file_size > 0:
                                percent = (downloaded / file_size) * 100
                                print(f"\r  Progress: {percent:.1f}%", end='', flush=True)
                    
                    print(f"\n‚úì Downloaded: {filename}")
                    return local_path
                
                elif response.status_code == 403:
                    print(f"  Access denied (file might not exist yet)")
                else:
                    print(f"  HTTP {response.status_code}")
            
            except requests.RequestException as e:
                print(f"  Download failed: {e}")
        
        print(f"‚ùå Could not download export file after {max_retries} attempts")
        return None
    
    def parse_export(self, filepath: str, filters: Optional[Dict] = None) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç NDJSON export —Ñ–∞–π–ª —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏.
        
        Args:
            filepath: –ü—É—Ç—å –∫ .json.gz —Ñ–∞–π–ª—É
            filters: –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –æ—Ç–±–æ—Ä–∞ ID, –Ω–∞–ø—Ä–∏–º–µ—Ä:
                {
                    "min_popularity": 10,
                    "exclude_adult": True,
                    "exclude_video": True  # –ø—Ä—è–º–æ–π —Ä–µ–ª–∏–∑ –Ω–∞ –≤–∏–¥–µ–æ
                }
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        filters = filters or {}
        min_popularity = filters.get("min_popularity", 0)
        exclude_adult = filters.get("exclude_adult", True)
        exclude_video = filters.get("exclude_video", True)
        
        results = []
        
        print(f"Parsing {os.path.basename(filepath)}...")
        
        with gzip.open(filepath, 'rt', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    item = json.loads(line.strip())
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                    if exclude_adult and item.get("adult", False):
                        continue
                    
                    if exclude_video and item.get("video", False):
                        continue
                    
                    popularity = item.get("popularity", 0)
                    if popularity < min_popularity:
                        continue
                    
                    results.append(item)
                
                except json.JSONDecodeError as e:
                    print(f"  Warning: Invalid JSON on line {line_num}: {e}")
                    continue
        
        print(f"‚úì Parsed {len(results)} valid items (from ~{line_num} total)")
        return results
    
    def get_filtered_ids(self, filters: Optional[Dict] = None, limit: Optional[int] = None) -> List[int]:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: —Å–∫–∞—á–∞—Ç—å -> —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å -> –≤–µ—Ä–Ω—É—Ç—å ID.
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –æ—Ç–±–æ—Ä–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ ID (–±–µ—Ä–µ–º top N –ø–æ popularity)
        
        Returns:
            –°–ø–∏—Å–æ–∫ TMDB ID
        """
        # 1. –°–∫–∞—á–∏–≤–∞–µ–º export
        filepath = self.download_export()
        if not filepath:
            raise RuntimeError("Failed to download ID export")
        
        # 2. –ü–∞—Ä—Å–∏–º —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
        items = self.parse_export(filepath, filters)
        
        # 3. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ popularity (descending)
        items.sort(key=lambda x: x.get("popularity", 0), reverse=True)
        
        # 4. –ë–µ—Ä–µ–º top N
        if limit:
            items = items[:limit]
        
        # 5. –ò–∑–≤–ª–µ–∫–∞–µ–º ID
        ids = [item["id"] for item in items]
        
        print(f"‚úì Selected {len(ids)} IDs")
        return ids


class MovieDetailsLoader(BaseLoader):
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–µ—Ç–∞–ª–µ–π —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–∏—Å–∫–∞ ID.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç IDExportLoader –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è ID, –∑–∞—Ç–µ–º –∫–∞—á–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —á–µ—Ä–µ–∑ API.
    """
    
    def __init__(self, target_count: int = 1000, min_popularity: float = 20):
        super().__init__()
        self.client = TMDBClient()
        self.id_loader = IDExportLoader(media_type="movie")
        self.target_count = target_count
        self.min_popularity = min_popularity
        self.target_locales = os.getenv("TARGET_LOCALES", "en,ru").split(",")
        
        # –ú–∞–ø–ø–∏–Ω–≥–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤
        self.genre_map = {}
        self.country_map = {}
    
    def _load_reference_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞"""
        print("  Loading reference data...")
        
        self.cursor.execute("SELECT id, genre FROM content_service.genres")
        self.genre_map = {row[1]: row[0] for row in self.cursor.fetchall()}
        
        self.cursor.execute("SELECT id, iso_code FROM content_service.countries")
        self.country_map = {row[1]: row[0] for row in self.cursor.fetchall()}
        
        print(f"  Loaded {len(self.genre_map)} genres, {len(self.country_map)} countries")
    
    def extract(self) -> List[Dict]:
        """
        1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ ID —á–µ—Ä–µ–∑ daily export (–±—ã—Å—Ç—Ä–æ)
        2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ ID —á–µ—Ä–µ–∑ API (–º–µ–¥–ª–µ–Ω–Ω–æ)
        """
        self._load_reference_data()
        
        # –®–∞–≥ 1: –ü–æ–ª—É—á–∏—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ ID
        print(f"\nüì• Fetching top {self.target_count} movie IDs (popularity >= {self.min_popularity})...")
        
        movie_ids = self.id_loader.get_filtered_ids(
            filters={
                "min_popularity": self.min_popularity,
                "exclude_adult": True,
                "exclude_video": True
            },
            limit=self.target_count
        )
        
        # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–µ—Ç–∞–ª–∏ —á–µ—Ä–µ–∑ API
        print(f"\nüì• Fetching details for {len(movie_ids)} movies...")
        print(f"‚ö†Ô∏è  This will take approximately {len(movie_ids) * 0.26 / 60:.1f} minutes")
        
        movies_data = []
        failed_ids = []
        
        from tqdm import tqdm
        
        for movie_id in tqdm(movie_ids, desc="Fetching movie details"):
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏
            details = self.client.get_movie_details(movie_id, language="en")
            if not details:
                failed_ids.append(movie_id)
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã
            translations = self.client.get_movie_translations(movie_id)
            
            movies_data.append({
                "details": details,
                "translations": translations
            })
        
        if failed_ids:
            print(f"\n‚ö†Ô∏è  Failed to fetch {len(failed_ids)} movies: {failed_ids[:10]}...")
        
        print(f"\n‚úì Successfully fetched {len(movies_data)} movies")
        return movies_data
    
    def transform(self, raw_data: List[Dict]) -> Dict[str, List]:
        """
        –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ë–î (–∫–∞–∫ –≤ MovieLoader).
        –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü.
        """
        content_data = []
        movie_details_data = []
        translations_data = []
        genres_data = []
        countries_data = []
        
        for movie in raw_data:
            details = movie["details"]
            tmdb_id = details["id"]
            
            # 1. content
            content_data.append((
                tmdb_id,
                details.get("original_title", "Unknown"),
                "movie",
                details.get("poster_path"),
                details.get("release_date"),
                "published",
                None,
                details.get("budget"),
                details.get("revenue")
            ))
            
            # 2. movie_details
            runtime = details.get("runtime")
            if runtime:
                movie_details_data.append((
                    tmdb_id,
                    runtime,
                    details.get("release_date"),
                    None
                ))
            
            # 3. translations
            for translation in movie["translations"]:
                iso_639_1 = translation.get("iso_639_1")
                data = translation.get("data", {})
                title = data.get("title") or details.get("original_title")
                overview = data.get("overview")
                
                if iso_639_1 in self.target_locales:
                    translations_data.append((
                        tmdb_id,
                        iso_639_1,
                        title,
                        overview,
                        None
                    ))
            
            # 4. genres
            for idx, genre in enumerate(details.get("genres", [])):
                genre_name = genre["name"]
                if genre_name in self.genre_map:
                    genres_data.append((
                        tmdb_id,
                        self.genre_map[genre_name],
                        idx
                    ))
            
            # 5. countries
            for country in details.get("production_countries", []):
                iso_code = country["iso_3166_1"]
                if iso_code in self.country_map:
                    countries_data.append((
                        tmdb_id,
                        self.country_map[iso_code]
                    ))
        
        return {
            "content": content_data,
            "movie_details": movie_details_data,
            "translations": translations_data,
            "genres": genres_data,
            "countries": countries_data
        }
    
    def get_upsert_query(self) -> str:
        pass  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
    
    def run(self):
        """–ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        print(f"\n{'='*60}")
        print(f"Starting {self.__class__.__name__}")
        print(f"Target: {self.target_count} movies (min popularity: {self.min_popularity})")
        print(f"{'='*60}\n")
        
        with self:
            raw_data = self.extract()
            if not raw_data:
                print("‚ö†Ô∏è  No data extracted")
                return
            
            print("\n‚öôÔ∏è  Transforming data...")
            transformed = self.transform(raw_data)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
            self._load_table("content", transformed["content"], """
                INSERT INTO content_service.content 
                (id, original_title, content_type, poster_url, release_date, status, age_rating, budget, box_office)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (id) DO UPDATE SET
                    original_title = EXCLUDED.original_title,
                    poster_url = EXCLUDED.poster_url,
                    release_date = EXCLUDED.release_date,
                    budget = EXCLUDED.budget,
                    box_office = EXCLUDED.box_office,
                    updated_at = NOW()
            """)
            
            self._load_table("movie_details", transformed["movie_details"], """
                INSERT INTO content_service.movie_details 
                (content_id, duration_minutes, cinema_release_date, digital_release_date)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (content_id) DO UPDATE SET
                    duration_minutes = EXCLUDED.duration_minutes,
                    cinema_release_date = EXCLUDED.cinema_release_date,
                    updated_at = NOW()
            """)
            
            self._load_table("translations", transformed["translations"], """
                INSERT INTO content_service.content_translations 
                (content_id, locale, title, description, plot_summary)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (content_id, locale) DO UPDATE SET
                    title = EXCLUDED.title,
                    description = EXCLUDED.description,
                    updated_at = NOW()
            """)
            
            self._load_table("genres", transformed["genres"], """
                INSERT INTO content_service.content_genres (content_id, genre_id, display_order)
                VALUES (%s, %s, %s)
                ON CONFLICT (content_id, genre_id) DO UPDATE SET
                    display_order = EXCLUDED.display_order
            """)
            
            self._load_table("countries", transformed["countries"], """
                INSERT INTO content_service.content_countries (content_id, country_id)
                VALUES (%s, %s)
                ON CONFLICT (content_id, country_id) DO NOTHING
            """)
        
        print(f"\n‚úÖ {self.__class__.__name__} completed successfully\n")
    
    def _load_table(self, name: str, data: List, query: str):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
        if not data:
            print(f"  No {name} to load")
            return
        
        from psycopg2.extras import execute_batch
        from tqdm import tqdm
        
        print(f"\nüì§ Loading {name}...")
        with tqdm(total=len(data), desc=f"Loading {name}") as pbar:
            for i in range(0, len(data), self.batch_size):
                batch = data[i:i + self.batch_size]
                execute_batch(self.cursor, query, batch, page_size=self.batch_size)
                pbar.update(len(batch))
        
        print(f"  ‚úì Loaded {len(data)} {name}")


if __name__ == "__main__":
    # –¢–µ—Å—Ç: –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–ø-100 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤
    loader = MovieDetailsLoader(target_count=100, min_popularity=20)
    loader.run()